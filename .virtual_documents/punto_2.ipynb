# librerias para manejo de datos
use_plotly = False
import scipy
import random
import numpy as np
import pandas as pd
import warnings
import seaborn as sns
import matplotlib.pyplot as plt
from plotly import figure_factory as ff
from plotly.subplots import make_subplots
if use_plotly:
    import plotly.graph_objects as go
    import plotly.express as px
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.impute import SimpleImputer
from sklearn.inspection import permutation_importance
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from minisom import MiniSom
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import numpy as np


train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/avila-tr.csv')
test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/avila-ts.csv')


train.head()


train.info()


train.describe()


test.isnull().sum()


train['class'].value_counts(normalize=True)


# Obtener conteos de clases
class_counts = train['class'].value_counts()

# Crear figura y ejes
plt.figure(figsize=(10, 6))

# Crear gráfico de barras
bars = plt.bar(class_counts.index, class_counts.values)

# Personalizar el gráfico
plt.title('Ocurrencia de cada clase', pad=20)
plt.xlabel('Clase')
plt.ylabel('Ocurrencia')

# Añadir valores sobre las barras
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
            f'{int(height)}',
            ha='center', va='bottom')

# Ajustar layout
plt.tight_layout()

# Mostrar gráfico
plt.show()


combined_df = pd.concat([train, test], ignore_index=True)
display(combined_df['class'].value_counts(normalize=True))


# Obtener conteos de clases
class_counts = combined_df['class'].value_counts()

# Crear figura y ejes
plt.figure(figsize=(10, 6))

# Crear gráfico de barras
bars = plt.bar(class_counts.index, class_counts.values)

# Personalizar el gráfico
plt.title('Ocurrencia de cada clase', pad=20)
plt.xlabel('Clase')
plt.ylabel('Ocurrencia')

# Añadir valores sobre las barras
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
            f'{int(height)}',
            ha='center', va='bottom')

# Ajustar layout
plt.tight_layout()

# Mostrar gráfico
plt.show()


display(combined_df['class'].value_counts())





class_mapping = {
    'A': 'A',
    'F': 'F',
    'E': 'E',
    'I': 'I',
    'X': 'Rare',
    'H': 'Rare',
    'G': 'Rare',
    'D': 'Rare',
    'O': 'Rare',
    'C': 'Rare',
    'W': 'Rare',
    'B': 'Rare'
}
display(class_mapping)





combined_df['merged_class'] = combined_df['class'].map(class_mapping)
display(combined_df[['class', 'merged_class']].head())


display(combined_df['merged_class'].value_counts())
display(combined_df['merged_class'].value_counts(normalize=True))

plt.figure(figsize=(10, 6))
bars = plt.bar(combined_df['merged_class'].value_counts().index, combined_df['merged_class'].value_counts().values)
plt.title('Ocurrence of each merged class', pad=20)
plt.xlabel('Merged Class')
plt.ylabel('Ocurrence')
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
            f'{int(height)}',
            ha='center', va='bottom')
plt.tight_layout()
plt.show()


# Crear figura y ejes con un tamaño adecuado para mostrar todas las características
plt.figure(figsize=(15, 6))

# Obtener las columnas numéricas (todas excepto 'class')
numeric_cols = combined_df.select_dtypes(include=['float64', 'int64']).columns

# Crear el boxplot
bp = plt.boxplot([combined_df[col] for col in numeric_cols], labels=numeric_cols)

# Personalizar el gráfico
plt.title('Distribución de características en el dataset Ávila', pad=20)
plt.xticks(rotation=45)  # Rotar etiquetas para mejor legibilidad
plt.ylabel('Valor')

# Ajustar márgenes para evitar corte de etiquetas
plt.tight_layout()

# Mostrar el gráfico
plt.show()


#esto sirve al final
pairpl = sns.pairplot(combined_df, hue='class', diag_kind='kde', corner=True);
pairpl._legend.remove()


corr = combined_df.corr(numeric_only=True)
plt.figure(figsize=(10,8))
sns.heatmap(corr, cmap="coolwarm", annot=False)
plt.show()


from sklearn.model_selection import train_test_split

# Separate features (X) and target (y)
X = combined_df.drop(['class', 'merged_class'], axis=1)
y = combined_df['merged_class']

# Split into 80% for temporary_train and 20% for test+validation
X_temp_train, X_test, y_temp_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Split the 20% into 10% test and 10% validation
X_train, X_val, y_train, y_val = train_test_split(X_temp_train, y_temp_train, test_size=0.125, random_state=42, stratify=y_temp_train) # 0.125 of 80% is 10% of original

# Display the shapes of the resulting splits
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of X_val:", X_val.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)
print("Shape of y_val:", y_val.shape)



# Initialize the scaler
scaler = StandardScaler()

# Fit the scaler on the training data and transform the training, validation, and test sets
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Display the shapes of the scaled data
print("Shape of X_train_scaled:", X_train_scaled.shape)
print("Shape of X_val_scaled:", X_val_scaled.shape)
print("Shape of X_test_scaled:", X_test_scaled.shape)





# Método de la Silueta para encontrar el número óptimo de clusters
silhouette_scores = []
k_range_silhouette = range(2, 11)  # El coeficiente de silueta requiere al menos 2 clusters

for k in k_range_silhouette:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_train_scaled)
    score = silhouette_score(X_train_scaled, kmeans.labels_)
    silhouette_scores.append(score)

# Graficar los resultados del método de la silueta
plt.figure(figsize=(8, 4))
plt.plot(k_range_silhouette, silhouette_scores, marker='o')
plt.xlabel('Número de Clusters (k)')
plt.ylabel('Coeficiente de Silueta')
plt.title('Método de la Silueta para la Estimación de k')
plt.xticks(k_range_silhouette)
plt.grid(True)
plt.show()


# Método del Codo para encontrar el número óptimo de clusters
inertia = []
k_range = range(1, 11)  # Puedes ajustar este rango según tus datos

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) # Explicitly set n_init
    kmeans.fit(X_train_scaled)
    inertia.append(kmeans.inertia_)

# Graficar los resultados del método del codo
plt.figure(figsize=(8, 4))
plt.plot(k_range, inertia, marker='o')
plt.xlabel('Número de Clusters (k)')
plt.ylabel('Inercia')
plt.title('Método del Codo para la Estimación de k')
plt.xticks(k_range)
plt.grid(True)
plt.show()


# Crear figura y ejes con un tamaño adecuado para mostrar todas las características
plt.figure(figsize=(15, 6))

# Obtener las columnas numéricas (todas excepto 'class')
numeric_cols = combined_df.select_dtypes(include=['float64', 'int64']).columns

# Crear el boxplot
bp = plt.boxplot([combined_df[col] for col in numeric_cols], tick_labels=numeric_cols)

# Personalizar el gráfico
plt.title('Distribución de características en el dataset Ávila', pad=20)
plt.xticks(rotation=45)  # Rotar etiquetas para mejor legibilidad
plt.ylabel('Valor')

# Ajustar márgenes para evitar corte de etiquetas
plt.tight_layout()

# Mostrar el gráfico
plt.show()





outlier_indices = {}

for col in numeric_cols:
    Q1 = combined_df[col].quantile(0.25)
    Q3 = combined_df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    col_outlier_indices = combined_df[(combined_df[col] < lower_bound) | (combined_df[col] > upper_bound)].index
    outlier_indices[col] = col_outlier_indices

# You can aggregate the indices if needed, for now, we just print the number of outliers per column
for col, indices in outlier_indices.items():
    print(f"Column '{col}': {len(indices)} outliers identified.")





numeric_cols = combined_df.select_dtypes(include=['float64', 'int64']).columns

for col in numeric_cols:
    Q1 = combined_df[col].quantile(0.25)
    Q3 = combined_df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    combined_df[col] = combined_df[col].clip(lower=lower_bound, upper=upper_bound)

display(combined_df.describe())





# Re-calculate the inertia for a range of cluster numbers
inertia = []
k_range = range(1, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_train_scaled)
    inertia.append(kmeans.inertia_)

# Plot the Elbow method results
plt.figure(figsize=(8, 4))
plt.plot(k_range, inertia, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Estimating k (After Outlier Handling)')
plt.xticks(k_range)
plt.grid(True)
plt.show()

# Re-calculate silhouette scores for a range of cluster numbers
silhouette_scores = []
k_range_silhouette = range(2, 11)

for k in k_range_silhouette:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_train_scaled)
    score = silhouette_score(X_train_scaled, kmeans.labels_)
    silhouette_scores.append(score)

# Plot the Silhouette method results
plt.figure(figsize=(8, 4))
plt.plot(k_range_silhouette, silhouette_scores, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Coefficient')
plt.title('Silhouette Method for Estimating k (After Outlier Handling)')
plt.xticks(k_range_silhouette)
plt.grid(True)
plt.show()





# Choose optimal k based on the re-evaluation (choosing k=4)
optimal_k = 4

# Instantiate KMeans
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)

# Fit KMeans to scaled training data
kmeans.fit(X_train_scaled)

# Predict cluster labels for train, validation, and test sets
train_cluster_labels = kmeans.predict(X_train_scaled)
val_cluster_labels = kmeans.predict(X_val_scaled)
test_cluster_labels = kmeans.predict(X_test_scaled)

# Store predicted cluster labels in combined_df
# Need to create a temporary series for each split with the correct index
train_labels_series = pd.Series(train_cluster_labels, index=X_train.index)
val_labels_series = pd.Series(val_cluster_labels, index=X_val.index)
test_labels_series = pd.Series(test_cluster_labels, index=X_test.index)

# Assign the labels back to the original combined_df using the indices
combined_df['kmeans_cluster_label'] = pd.NA
combined_df.loc[train_labels_series.index, 'kmeans_cluster_label'] = train_labels_series
combined_df.loc[val_labels_series.index, 'kmeans_cluster_label'] = val_labels_series
combined_df.loc[test_labels_series.index, 'kmeans_cluster_label'] = test_labels_series

# Convert the new column to integer type, ignoring NaNs which will be handled by fillna
combined_df['kmeans_cluster_label'] = combined_df['kmeans_cluster_label'].astype(int)

display(combined_df[['class', 'merged_class', 'kmeans_cluster_label']].head())
display(combined_df['kmeans_cluster_label'].value_counts())





# 1. Calculate Silhouette Score for training data
train_silhouette_score = silhouette_score(X_train_scaled, train_cluster_labels)

# 2. Calculate Silhouette Score for validation data
val_silhouette_score = silhouette_score(X_val_scaled, val_cluster_labels)

# 3. Calculate Silhouette Score for test data
test_silhouette_score = silhouette_score(X_test_scaled, test_cluster_labels)

# 4. Print Silhouette Scores
print(f"Silhouette Score (Training): {train_silhouette_score}")
print(f"Silhouette Score (Validation): {val_silhouette_score}")
print(f"Silhouette Score (Test): {test_silhouette_score}")

# 5. Create a confusion matrix or a cross-tabulation
confusion_matrix_kmeans = pd.crosstab(combined_df['merged_class'], combined_df['kmeans_cluster_label'])

# 6. Display the confusion matrix or cross-tabulation
print("\nConfusion Matrix: Merged Class vs KMeans Cluster Label")
display(confusion_matrix_kmeans)

# 7. Briefly interpret the results
print("\nInterpretation:")
print(f"- The Silhouette Scores (Training: {train_silhouette_score:.3f}, Validation: {val_silhouette_score:.3f}, Test: {test_silhouette_score:.3f}) are relatively low, suggesting that the clusters are not well-separated.")
print("- The confusion matrix shows how the merged classes distribute across the K-Means clusters. Ideally, each merged class would primarily map to a single cluster. The matrix indicates some overlap between classes and clusters, with the 'A' class being spread across multiple clusters, while 'F', 'E', 'I', and 'Rare' classes show some concentration in specific clusters.")
print("- The clustering results do not perfectly align with the merged classes, suggesting that the features might not clearly separate these merged classes based on K-Means clustering with k=4.")


# Create a pairplot colored by the K-Means cluster labels
# We'll use a subset of the data for faster plotting, or select a few key features
# to avoid a very large plot if the number of features is high.
# For this dataset with 10 features, a full pairplot might be too slow or cluttered.
# Let's select a few features for visualization.

# Select a subset of features for the pairplot
# You can change these features based on your interest
features_for_plotting = ['intercolumnar_distance', 'upper_margin', 'exploitation', 'weight', 'peak_number']

# Create a temporary DataFrame with selected features and cluster labels
plot_df = combined_df[features_for_plotting].copy()
plot_df['kmeans_cluster_label'] = combined_df['kmeans_cluster_label']

# Create the pairplot
pairpl = sns.pairplot(plot_df, hue='kmeans_cluster_label', diag_kind='kde', corner=True, palette='viridis');

# Customize the plot title (optional)
pairpl.fig.suptitle('Pairplot of Selected Features Colored by KMeans Clusters', y=1.02) # Adjust title position

# Show the plot
plt.show()











# ---------- 1) Datos ----------
# Use the scaled training data X_train_scaled for SOM
X = X_train_scaled
N, d = X.shape

# Define SOM grid size - adjust as needed. A common heuristic is sqrt(N)
# For N=14606, sqrt(14606) is approx 120. An 8x8 grid might be too small.
# Let's try a larger grid, e.g., 20x20 or 30x30. Let's start with 20x20.
m = n = 20


# ---------- 2) Entrenar SOM ----------
# Adjust sigma and learning_rate if needed based on your data and desired convergence
som = MiniSom(x=m, y=n, input_len=d, sigma=5.0, learning_rate=0.5,
              neighborhood_function='gaussian', random_seed=7)
som.random_weights_init(X)
# Increase training iterations for a larger dataset and grid
som.train_random(X, num_iteration=10000)


# ---------- 3) U-Matrix y Hit map ----------
U = som.distance_map()           # (m,n)
hits = np.zeros((m, n), dtype=int)
for x in X:
    i, j = som.winner(x)
    hits[i, j] += 1

plt.figure(figsize=(10, 8)); plt.pcolor(U.T, cmap='bone_r'); plt.colorbar(); plt.title('U-Matrix (MiniSom)')
plt.figure(figsize=(10, 8)); plt.pcolor(U.T, cmap='bone_r')
for x in X:
    i, j = som.winner(x); plt.plot(i+0.5, j+0.5, 'o', markersize=2, color='darkblue', alpha=0.6)
plt.title('Hit map sobre U-Matrix')


# ---------- 4) Selección de K en prototipos ----------
W = som.get_weights().reshape(m*n, d)     # (m*n, d)
Ks = range(2, 11) # Adjust range as needed
sil = []
labels_by_K = {}
for K in Ks:
    # Use n_init='auto' or a specific number like 20
    km = KMeans(n_clusters=K, n_init=20, random_state=7)
    lab = km.fit_predict(W)
    sil.append(silhouette_score(W, lab))
    labels_by_K[K] = lab

plt.figure(figsize=(8, 4)); plt.plot(list(Ks), sil, marker='o')
plt.xticks(list(Ks)); plt.xlabel('K'); plt.ylabel('Silhouette media')
plt.title('Selección de K sobre prototipos del SOM')


# Select the K with the highest silhouette score
K_star = list(Ks)[int(np.argmax(sil))]
lab_star = labels_by_K[K_star]

plt.figure(figsize=(10, 8))
plt.scatter(W[:,0], W[:,1], c=lab_star, s=20, cmap='viridis')
plt.title(f'Prototipos SOM coloreados por clusters (K={K_star})')
# SOM prototypes are in the high-dimensional feature space, not necessarily 2D
# If d > 2, visualizing with just two components (W[:,0], W[:,1]) might not be representative.
# Consider using PCA or selecting key features for visualization if d > 2.
if d > 2:
    print(f"Note: The prototypes are in {d}-dimensional space. Visualizing with only the first two components.")
    plt.xlabel('Component 1'); plt.ylabel('Component 2')
else:
     plt.xlabel('Feature 1'); plt.ylabel('Feature 2')




# ---------- 5) Rejilla de prototipos sobre datos (Optional - challenging for high dimensions) ----------
# This visualization is typically useful for 2D data. For d > 2, it's less informative.
# If you want to visualize the grid, you might need to reduce the dimensions of the data and weights first (e.g., using PCA)

plt.figure(figsize=(10, 8))
plt.scatter(X[:,0], X[:,1], s=8, alpha=0.4)
Wg = som.get_weights()
plt.plot(Wg[:,:,0].reshape(-1, 1), Wg[:,:,1].reshape(-1, 1), 'k-', alpha=0.6)
plt.plot(Wg[:,:,0].T.reshape(-1, 1), Wg[:,:,1].T.reshape(-1, 1), 'k-', alpha=0.6)
plt.scatter(W[:,0], W[:,1], s=12, color='red')
plt.title('Datos normalizados y rejilla de prototipos (2D Projection)')
if d > 2:
    plt.xlabel('Component 1 (e.g., from PCA)'); plt.ylabel('Component 2 (e.g., from PCA)')
else:
     plt.xlabel('z(feature_1)'); plt.ylabel('z(feature_2)')
plt.show()







# Map data points to their winning SOM node
winning_nodes = np.array([som.winner(x) for x in X_train_scaled]).T
winning_nodes_flat = np.ravel_multi_index(winning_nodes, (m, n))

# Assign merged_class labels to SOM nodes based on majority class of winning data points
node_class_mapping = {}
for i in range(m * n):
    # Find the indices of data points that map to this node
    data_indices_for_node = X_train.index[winning_nodes_flat == i]
    if len(data_indices_for_node) > 0:
        # Get the merged classes for these data points
        classes_for_node = y_train.loc[data_indices_for_node]
        # Find the majority class
        majority_class = classes_for_node.mode()
        if not majority_class.empty:
            node_class_mapping[i] = majority_class[0]
        else:
            node_class_mapping[i] = 'Unknown' # Handle cases with no majority

# Assign the learned merged_class from the SOM node to each training data point
train_som_assigned_class = pd.Series(index=X_train.index, dtype=object)
for idx in X_train.index:
    i, j = som.winner(X_train_scaled[X_train.index.get_loc(idx)]) # Get winner for the original index
    node_flat_idx = np.ravel_multi_index((i, j), (m, n))
    train_som_assigned_class.loc[idx] = node_class_mapping.get(node_flat_idx, 'Unknown')


# Create a confusion matrix comparing actual merged_class and SOM assigned class for training data
confusion_matrix_som = pd.crosstab(y_train, train_som_assigned_class, dropna=False)

print("\nConfusion Matrix: Merged Class vs SOM Assigned Class (Training Data)")
display(confusion_matrix_som)

# Note: Evaluating SOM cluster quality rigorously often involves more complex metrics
# and might require assigning K-Means clusters to the SOM prototypes and then mapping those back.
# This confusion matrix gives an idea of how the SOM's topology aligns with the merged classes.








# Display the previously calculated confusion matrix for K-Means
print("\nConfusion Matrix: Merged Class vs KMeans Cluster Label (All Data)")
display(confusion_matrix_kmeans)

# Note: The K-Means confusion matrix was generated using the assigned clusters on the entire combined_df,
# while the SOM matrix is generated using assignments based on the training data and prototype mapping.
# Direct comparison should take this into account.



