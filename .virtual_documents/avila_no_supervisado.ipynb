# librerias para manejo de datos
use_plotly = False
import scipy
import random
import numpy as np
import pandas as pd
import warnings
import seaborn as sns
import matplotlib.pyplot as plt
from plotly import figure_factory as ff
from plotly.subplots import make_subplots
if use_plotly:
    import plotly.graph_objects as go
    import plotly.express as px
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.impute import SimpleImputer
from sklearn.inspection import permutation_importance
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score





train = pd.read_csv('avila-tr.csv')
test = pd.read_csv('avila-ts.csv')


train.head()


train.info()


train.describe()


test.isnull().sum()


test.head()


test.info()


test.describe()


train['class'].value_counts(normalize=True)


test['class'].value_counts(normalize=True)


# Obtener conteos de clases
class_counts = train['class'].value_counts()

# Crear figura y ejes
plt.figure(figsize=(10, 6))

# Crear gráfico de barras
bars = plt.bar(class_counts.index, class_counts.values)

# Personalizar el gráfico
plt.title('Ocurrencia de cada clase', pad=20)
plt.xlabel('Clase')
plt.ylabel('Ocurrencia')

# Añadir valores sobre las barras
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
            f'{int(height)}',
            ha='center', va='bottom')

# Ajustar layout
plt.tight_layout()

# Mostrar gráfico
plt.show()


# Identificar las clases con menor cantidad de datos
clases_minoritarias = ['Y', 'C', 'W', 'B']

# Reemplazar las clases minoritarias por una nueva clase 'O'
train['class'] = train['class'].replace(clases_minoritarias, 'O')
test['class'] = test['class'].replace(clases_minoritarias, 'O')


test.describe()


# Crear figura y ejes con un tamaño adecuado para mostrar todas las características
plt.figure(figsize=(15, 6))

# Obtener las columnas numéricas (todas excepto 'class')
numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns

# Crear el boxplot
bp = plt.boxplot([train[col] for col in numeric_cols], labels=numeric_cols)

# Personalizar el gráfico
plt.title('Distribución de características en el dataset Ávila', pad=20)
plt.xticks(rotation=45)  # Rotar etiquetas para mejor legibilidad
plt.ylabel('Valor')

# Ajustar márgenes para evitar corte de etiquetas
plt.tight_layout()

# Mostrar el gráfico
plt.show()


#esto sirve al final 
pairpl = sns.pairplot(test, hue='class', diag_kind='kde', corner=True);
pairpl._legend.remove()


train.corr(numeric_only=True)


corr = train.corr(numeric_only=True)
plt.figure(figsize=(10,8))
sns.heatmap(corr, cmap="coolwarm", annot=False)
plt.show()


train.groupby("class")["weight"].mean().sort_values()


# Ejemplo comparando dos clases
# Calcular la media de las características agrupadas por clase
class_means = train.groupby('class').mean()
clases = ["A","F","E","I","X", "H"]
vars = class_means.columns
angles = np.linspace(0, 2*np.pi, len(vars), endpoint=False).tolist()

fig, ax = plt.subplots(figsize=(6,6), subplot_kw=dict(polar=True))
for c in clases:
    valores = class_means.loc[c].values
    valores = np.concatenate((valores,[valores[0]]))  # cerrar círculo
    ax.plot(angles + [angles[0]], valores, label=f"Clase {c}")
    ax.fill(angles + [angles[0]], valores, alpha=0.25)
ax.set_xticks(angles)
ax.set_xticklabels(vars, rotation=20)
ax.legend()
plt.show()














































# Añadir esta celda para el manejo de outliers
print("Forma del dataset de entrenamiento antes de eliminar outliers:", train.shape)

# Seleccionar solo columnas numéricas para el tratamiento de outliers
numeric_cols = train.select_dtypes(include=np.number).columns.tolist()

for col in numeric_cols:
    Q1 = train[col].quantile(0.25)
    Q3 = train[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Filtrar los outliers
    train = train[(train[col] >= lower_bound) & (train[col] <= upper_bound)]

print("Forma del dataset de entrenamiento después de eliminar outliers:", train.shape)


# Antes de aplicar K-Means
X_train = train.drop('class', axis=1)
y_train = train['class']

X_test = test.drop('class', axis=1)
y_test = test['class']


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


inertia = []
k_range = range(1, 15) # Probaremos con K de 1 a 14

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_train_scaled)
    inertia.append(kmeans.inertia_)

# Graficar los resultados
plt.figure(figsize=(10, 6))
plt.plot(k_range, inertia, marker='o')
plt.title('Método del Codo para encontrar K óptimo')
plt.xlabel('Número de clústeres (K)')
plt.ylabel('Inercia')
plt.xticks(k_range)
plt.grid(True)
plt.show()


silhouette_scores = []
# El rango de K debe empezar en 2, ya que la puntuación de silueta no se puede calcular para un solo clúster.
k_range = range(2, 15) 

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    # Usamos fit_predict para obtener las etiquetas de los clústeres directamente
    cluster_labels = kmeans.fit_predict(X_train_scaled)
    
    # Calculamos la puntuación de silueta promedio
    score = silhouette_score(X_train_scaled, cluster_labels)
    silhouette_scores.append(score)
    print(f"Para K={k}, la puntuación de silueta es: {score:.4f}")

# Graficar los resultados
plt.figure(figsize=(10, 6))
plt.plot(k_range, silhouette_scores, marker='o')
plt.title('Análisis de Silueta para encontrar K óptimo')
plt.xlabel('Número de clústeres (K)')
plt.ylabel('Puntuación de Silueta Promedio')
plt.xticks(k_range)
plt.grid(True)
plt.show()


# Añadir esta celda para entrenar el modelo final
optimal_k = 4 # Reemplaza con el valor que encontraste
kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
train_clusters = kmeans_final.fit_predict(X_train_scaled)

# Añadir los clústeres al DataFrame original para análisis
train_with_clusters = train.copy()
train_with_clusters['cluster'] = train_clusters

pd.Series(train_with_clusters['cluster']).value_counts().sort_index()


# Crear una tabla de contingencia
contingency_table = pd.crosstab(train_with_clusters['class'], train_with_clusters['cluster'])

print("Tabla de Contingencia (Clases vs. Clústeres):")
contingency_table



pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_scaled)

plt.figure(figsize=(12, 8))
sns.scatterplot(x=X_train_pca[:, 0], y=X_train_pca[:, 1], hue=train_clusters, palette='viridis', s=50, alpha=0.7)
plt.title('Visualización de Clústeres con PCA')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.legend(title='Clúster')
plt.show()


# Puntos coloreados por clúster
plt.figure(figsize=(7,6))
plt.scatter(X_train_pca[:,0], X_train_pca[:,1],
            c=train_labels, s=3, alpha=0.35)
# Centroides en PC1–PC2
C2 = kmeans.cluster_centers_[:, :2]
plt.scatter(C2[:,0], C2[:,1], s=200, marker='X', edgecolor='k')
for i, (x, y) in enumerate(C2):
    plt.text(x, y, f"C{i}", fontsize=10, ha='center', va='bottom')

plt.xlabel("PC1"); plt.ylabel("PC2")
plt.title(f"Puntos y centroides en PCA (k={kmeans.n_clusters})")
plt.tight_layout()
plt.show()























#
# PCA con todos los componentes
pca_full = PCA(n_components=10)
pca_full.fit(X_train_scaled)

# Varianza explicada
var_exp = pca_full.explained_variance_ratio_
cum_var_exp = np.cumsum(var_exp)

# Graficar
plt.figure(figsize=(8,5))
plt.plot(range(1, 11), cum_var_exp, marker='o')
plt.xlabel("Número de componentes")
plt.ylabel("Varianza acumulada")
plt.title("Scree Plot - Ávila dataset")
plt.grid(True)
plt.show()


pca = PCA(n_components=5)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

print("Varianza explicada individual:", pca.explained_variance_ratio_)
print("Varianza acumulada:", sum(pca.explained_variance_ratio_))


# Lista de pares de componentes a graficar
pairs = [(0,1), (1,2), (2,3), (0,2)]

plt.figure(figsize=(12,10))

for i, (a,b) in enumerate(pairs, 1):
    plt.subplot(2,2,i)
    plt.scatter(X_train_pca[:,a], X_train_pca[:,b], s=2, alpha=0.5)
    plt.xlabel(f"PC{a+1}")
    plt.ylabel(f"PC{b+1}")
    plt.title(f"Ávila dataset: PC{a+1} vs PC{b+1}")

plt.tight_layout()
plt.show()



# Probar varios valores de k
inertias = []
silhouettes = []
K = range(2, 11)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_train_pca)
    inertias.append(kmeans.inertia_)
    silhouettes.append(silhouette_score(X_train_pca, labels))

# Gráfico del codo (inertia)
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(K, inertias, marker='o')
plt.xlabel("Número de clusters k")
plt.ylabel("Inertia")
plt.title("Método del codo")

# Gráfico del silhouette
plt.subplot(1,2,2)
plt.plot(K, silhouettes, marker='o')
plt.xlabel("Número de clusters k")
plt.ylabel("Silhouette")
plt.title("Índice de silhouette")
plt.show()


# 1) Buscar k óptimo en un rango razonable
cand_k = range(5, 9)  # p.ej., 3–8
inertias, silhouettes = [], []

for k in cand_k:
    km = KMeans(n_clusters=k, n_init='auto', random_state=42)
    labels = km.fit_predict(X_train_pca)
    inertias.append(km.inertia_)
    silhouettes.append(silhouette_score(X_train_pca, labels))

# Elegir k por mejor silhouette dentro del rango
best_k = cand_k[int(np.argmax(silhouettes))]
best_k, list(zip(cand_k, inertias, silhouettes))


# 2) Ajuste final con k óptimo
kmeans = KMeans(n_clusters=best_k, n_init='auto', random_state=42)
train_labels = kmeans.fit_predict(X_train_pca)

# 3) Tamaños de clúster
pd.Series(train_labels).value_counts().sort_index()


# 4) Caracterizar clústeres en variables originales
#    – promedio por clúster en el espacio original (sin estandarizar)
train_df = pd.DataFrame(X_train, columns=[f'feat_{i+1}' for i in range(X_train.shape[1])])
train_df['cluster'] = train_labels
cluster_profile = train_df.groupby('cluster').mean().round(3)
cluster_profile


# 5) Centros de clúster interpretables:
#    (a) centroide en PCA-space
centroids_pca = kmeans.cluster_centers_

#    (b) llevar centroides de vuelta a espacio estandarizado y luego original
#        (requiere objetos 'pca' y 'scaler' ya ajustados con n_components=5)
centroids_std = pca.inverse_transform(centroids_pca)          # vuelve a 10D estandarizado
centroids_orig = scaler.inverse_transform(centroids_std)      # vuelve a escala original

centroids = pd.DataFrame(centroids_orig,
                         columns=[f'feat_{i+1}' for i in range(X_train.shape[1])])
centroids.round(3)



plt.figure(figsize=(7,6))
plt.scatter(X_train_pca[:,0], X_train_pca[:,1], c=train_labels, s=3, alpha=0.6)
plt.xlabel("PC1"); plt.ylabel("PC2"); plt.title(f"K-Means en PCA (k={best_k})")
plt.show()



