from IPython.display import display
from IPython.display import HTML
import IPython.core.display as di # Example: di.display_html('<h3>%s:</h3>' % str, raw=True)

# This line will hide code by default when the notebook is exported as HTML
di.display_html("""
    <script>
        function show_code() {
            jQuery(".input_area").toggle(); 
            jQuery(".prompt").toggle();
            jQuery("#notebook-container").toggleClass("closed_notebook");
        }
    </script>
""", raw=True)

# This line will add a button to toggle visibility of code blocks, for use with the HTML export version
di.display_html('''<button onclick="show_code()">Show/hide code</button>''', raw=True)








use_plotly = False


import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
from plotly import figure_factory as ff
from plotly.subplots import make_subplots
from os.path import exists
from IPython.utils import io
from mlxtend.plotting import plot_decision_regions
from ast import literal_eval

if use_plotly:
    import plotly.graph_objects as go
    import plotly.express as px
    
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['axes.prop_cycle'] = plt.cycler(color=plt.cm.Dark2.colors)


from os.path import isdir
if not isdir('avila'):
  !curl 'https://archive.ics.uci.edu/ml/machine-learning-databases/00459/avila.zip' --output avila.zip
  !unzip avila.zip
  !rm avila.zip

train_path = 'avila/avila-tr.txt'
test_path = 'avila/avila-ts.txt'
random_state = 1153


names = [
  'intercolumnar distance',
  'upper margin',
  'lower margin',
  'exploitation',
  'row number',
  'modular ratio',
  'interlinear spacing',
  'weight',
  'peak number',
  'modular ratio/ interlinear spacing',
  'monk'
]

avila_tr = pd.read_csv(train_path, names=names)
avila_ts = pd.read_csv(test_path, names=names)
to_keep = ['A', 'F', 'E', 'X', 'I', 'H']

avila = avila_tr[avila_tr['monk'].isin(to_keep)]
# avila = avila.drop('modular ratio/ interlinear spacing', axis=1)

avila_test = avila_ts[avila_ts['monk'].isin(to_keep)]
# avila_test = avila_test.drop('modular ratio/ interlinear spacing', axis=1)

del avila_tr
del avila_ts

avila['monk'] = avila['monk'].astype('category')
avila_test['monk'] = avila_test['monk'].astype('category')
print('Dataset imported.')
print(f"Dataset size: {len(avila)}")
print(f'Dataset columns: {avila.shape[1]}')





print('Null values per attribute:')
print(np.sum(avila.isna()))





if use_plotly:
    class_bar = px.bar(avila['monk'].value_counts(),
           x=avila['monk'].value_counts().index,
           y=avila['monk'].value_counts(),
           color=avila['monk'].value_counts().index,
           text=avila['monk'].value_counts(),
           color_discrete_sequence = px.colors.qualitative.Antique,
           title=dict(text='Occurrence of each class', x=0.5, xanchor='center'),
           width=800, height=400,
          )

    class_bar.update_layout(showlegend=False, xaxis_title='Monk', yaxis_title='Occurences',  margin=dict(t=50, b=25, r=20), hovermode=False)
    class_bar.update_traces(textposition='outside')
    class_bar.show()








avila.describe()





def single_boxplot(df, colname) -> None:
  """
  Draws a single horizontal boxplot of the column 'colname' of the given dataframe.

  Args:
    df (pandas DataFrame): dataset
    colname (str): name of the column t plot
  """
  box = px.box(x=df[colname], orientation='h', height=200, color_discrete_sequence = px.colors.qualitative.Antique)
  box.update_layout(xaxis_title=colname, margin=dict(t=10, b=10, r=10, l=10), hovermode=False)
  box.show()


if use_plotly:
    single_boxplot(avila, 'upper margin')








new_std = avila['upper margin'][avila['upper margin'] < 50].std()
print(f"Standard deviation of attribute 'upper margin' with outlier removed: {new_std:.3}")
avila = avila[avila['upper margin'] < 50]





avila.describe()





def dataset_box_by_feature(ds) -> None:
  """
  Plot the boxplots of the given dataset, by column.

  Args:
    ds (pandas DataFrame): dataset to plot
  """
  tot_fig = go.Figure()
  for column in ds.columns:
    tot_fig.add_trace(
        go.Box(
          y=ds[column],
          name=column,
      )
    )

  tot_fig.update_layout(
      margin=dict(
          l=20,
          r=20,
          b=20,
          t=50,
          pad=0
      ),
      title={'text': f'Attributes of the dataset ({len(ds)} samples)', 'x': 0.5, 'xanchor': 'center'},
      showlegend=False
  )
  tot_fig.show()


if use_plotly:
    dataset_box_by_feature(avila)








corr = avila.corr()
hm_fig, hm_ax = plt.subplots(figsize=(12,9))
mask = np.logical_not(np.tril(np.ones_like(corr, dtype=np.bool)))
sns.heatmap(
    corr,
    annot=True,
    mask=mask,
    fmt='.3f',
    ax=hm_ax,
    cmap=sns.light_palette('#9c462f', n_colors=300),
)
hm_ax.set_xticklabels(hm_ax.get_xticklabels(), rotation=45)
hm_ax.set_title('Avila dataset correlation matrix')
hm_ax.patch.set_alpha(0);





pairpl = sns.pairplot(avila, hue='monk', diag_kind='kde', corner=True);
pairpl._legend.remove()





avila = avila.drop('modular ratio/ interlinear spacing', axis='columns')
avila_test = avila_test.drop('modular ratio/ interlinear spacing', axis='columns')

# Transfering everyting into an X and a y
X = avila.drop('monk', axis='columns')
y = avila['monk']





def dataset_rug(data, labels, additional_title=None):
  """
  Displays one rug plot for each different feature of the dataset. Each rug is divided by label.

  Args:
    data (pandas DataFrame): the dataset to display
    labels (pandas series or np.array): labels of the dataset
    additional_title (str or None): string to display in brackets after main title. If None, nothing is shown.
  """
  n_rows = int(np.ceil(len(data.columns)/2).astype(int))
  row_col = [(i,j) for i in range(1, n_rows+1) for j in (1,2)][:len(data.columns)]
  title = f'[{len(data)} points]'
  if additional_title:
    title = additional_title + ' ' + title

  fig = make_subplots(
      rows=n_rows, cols=2,
      horizontal_spacing=0.03,
      vertical_spacing=0.1,
      subplot_titles=(data.columns)
  )

  fig.update_layout(
      margin=dict(t=50, b=25, r=20, l=20),
      showlegend=False,
      title={'text': title, 'x': 0.5, 'xanchor': 'center'},
      height=900
  )

  for column, (row, col) in zip(data.columns, row_col):
    fig.add_trace(
        go.Scatter(
            x=data[column],
            y=labels,
            mode='markers',
            marker=go.scatter.Marker(
              color=labels.cat.codes,
              symbol='line-ns-open',
              colorscale=['red', 'green', 'blue']
          )
        ),
        row=row,
        col=col
    )
  
  fig.show()


if use_plotly:
    dataset_rug(X, y, additional_title='Full avila')








contamination = 200/len(X)


from sklearn.ensemble import IsolationForest

isol_for = IsolationForest(n_estimators=500, random_state=random_state, contamination=contamination, behaviour='new')
isol_for.fit(X)
s_scores = -isol_for.score_samples(X)





def simple_line(x, title_text='Line plot', y_label='y'):
  """
  Show a simple line graph of data x.
  """
  sl = px.line(
    y=x,
    title={'text': title_text, 'x': 0.5, 'xanchor': 'center'},
    width=700, height=400,
    color_discrete_sequence = px.colors.qualitative.Antique
  )
  sl.update_layout(xaxis_title='Sample', yaxis_title=y_label,  margin=dict(t=50, b=25, r=20))
  sl.update_traces(hovertemplate=y_label + ': %{y}<br>Number of samples: %{x}')
  sl.show()


if use_plotly:
    simple_line(-np.sort(-s_scores), 'Ordered s-score from Isolation Forest', 's-score')








s_threshold = 0.59
print(f'Using threshold {s_threshold}')
print(f'{y[s_scores > s_threshold].size} outilers found by Isolation Forest, with distibution')
print(y[s_scores > s_threshold].value_counts())





outliers_mask = (s_scores > s_threshold)
data = X[outliers_mask]
labels = y[outliers_mask]

if use_plotly:
    dataset_rug(data, labels, additional_title='Outliers caught by Isolation Forest')








from sklearn.neighbors import LocalOutlierFactor

lof = LocalOutlierFactor(contamination=contamination).fit(X)
lof_scores = -lof.negative_outlier_factor_


if use_plotly:
    simple_line(-np.sort(-lof_scores), title_text='Local Oulier Factor scores', y_label='LOF score')





lof_threshold = 1.68
print(f'Using threshold {lof_threshold}')
print(f'{y[lof_scores > lof_threshold].size} outilers found by LOF, with distibution')
print(y[lof_scores > lof_threshold].value_counts())


outliers_mask = (lof_scores > lof_threshold)
data = X[outliers_mask]
labels = y[outliers_mask]

if use_plotly:
    dataset_rug(data, labels, additional_title='Outliers caught by LOF')








n_common_outl = np.sum((lof_scores > lof_threshold) & (s_scores > s_threshold))
print(f'Number of common outilers between Isolation Forest and LOF: {n_common_outl}')





from sklearn.svm import OneClassSVM

ocsvm = OneClassSVM(nu=0.02)
out = ocsvm.fit_predict(X)
tot = np.sum(out == -1)
print(f'Outliers found by One Class SVM: {tot}')


outl_comm_svdd_isfor = np.sum((out == -1) & (s_scores > s_threshold))
print(f'Number of common outilers between Isolation Forest and One Class SVM (RBF): {outl_comm_svdd_isfor}')





outliers_mask = (out == -1)
data = X[outliers_mask]
labels = y[outliers_mask]

if use_plotly:
    dataset_rug(data, labels, additional_title='Outliers caught by One Class SVM (RBF kernel)')








points_to_keep = (s_scores <= s_threshold)
X = X[points_to_keep]
y = y[points_to_keep]
print(f'{len(X)} inliers left.')


data = avila[points_to_keep].drop('monk', axis='columns')
labels = avila[points_to_keep]['monk']

if use_plotly:
    dataset_rug(data, labels, additional_title='Dataset after outlier removal')








from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()








from scipy.stats import probplot
qqlabel = 'A'
figqq, axqq = plt.subplots(1, 2, figsize=(10,4))
probplot(X[y == qqlabel]['row number'], dist='norm', plot=axqq[0])
probplot(X[y == qqlabel]['interlinear spacing'], dist='norm', plot=axqq[1])
axqq[0].set_title(f"QQ plot of 'row number', class {qqlabel}")
axqq[1].set_title(f"QQ plot of 'interlinear spacing', class {qqlabel}");


from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.pipeline import Pipeline

X_qda = X.drop(['row number', 'interlinear spacing'], axis='columns')

lda_pipe = Pipeline([('standardizer', std_scaler), ('lda', LinearDiscriminantAnalysis())])
qda_pipe = Pipeline([('standardizer', std_scaler), ('qda', QuadraticDiscriminantAnalysis())])





from sklearn.model_selection import cross_val_predict

y_pred_qda = cross_val_predict(qda_pipe, X_qda, y, cv=5)
y_pred_lda = cross_val_predict(lda_pipe, X_qda, y, cv=5)


def dec_bound(clf, X, y, ax, columns=None,
              feature_index=(0,1), filler_val=0, filler_range=0.7,
              xlim=(-1, 1), ylim=(-1, 1)):
    """
    Plot decision regions of the selected classifier on X and y, and using axis ax.
    
    Params:
        clf: classifer to use (must be fitted)
        X (numpy array): data to plot
        y (numpy array): labels
        ax (matplotlib axis): ax on which to plot
        feature_index: which two features to include in the 2D plot
        filler_val: along with filler_range, determines which values of the excluded features to take into account. I don't understand that either, just take the default parameters. I carefully trial-and-error'd them.
        filler_range: see above
    """
    ft1 = feature_index[0]
    ft2 = feature_index[1]
    
    values = {num: filler_val for num in np.delete(np.arange(9), (ft1, ft2))}
    ranges = {num: filler_range for num in np.delete(np.arange(9), (ft1, ft2))}
    
    plot_decision_regions(
        X, y, clf, ax=ax, 
        feature_index=(ft1, ft2),
        filler_feature_values=values,
        filler_feature_ranges=ranges)
    
    ax.set_xlim(xlim)
    ax.set_ylim(ylim)
    
    if columns is not None:
        ax.set_xlabel(columns[ft1])
        ax.set_ylabel(columns[ft2])


# Code commented and replaced with a static jp image for faster rendering (let's call it "bruteforce approach")
# qd_fig, qd_ax = plt.subplots(1, 2, figsize=(13, 5.1))
# qd_ax[0].set_title('LDA decision boundaries')
# qd_ax[1].set_title('QDA decision boundaries')

# num_X = X.to_numpy()
# num_y = y.cat.codes.to_numpy()
# lda_pipe.fit(num_X, num_y)

# dec_bound(lda_pipe, num_X, num_y, ax=qd_ax[0], columns=X.columns, feature_index=(5,8), ylim=(0,2), filler_val=0.5, filler_range=0.53)
# dec_bound(qda_pipe, num_X, num_y, ax=qd_ax[1], columns=X.columns, feature_index=(5,8), ylim=(0,2), filler_val=0.5, filler_range=0.53)








from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

print(f'Results with LDA:\n{classification_report(y, y_pred_lda)}\n')
print(f'Results with QDA:\n{classification_report(y, y_pred_qda)}')
baseline = accuracy_score(y, y_pred_qda)
print(f'Baseline: {baseline:.3}')





from sklearn.metrics import f1_score

def show_f1s(y, y_pred, title='f1 scores and support'):
    """
    Plot a double bar chart of the f1 scores for each class and their support. Sort them in descending order of support.
    
    Params:
        y: true labels
        y_pred: predicted labels
        title: optional title of the plot
    """
    # Obtain f1s, put them in pandas dataframe
    f1 = f1_score(y, y_pred, average=None)
    classes = y.cat.categories # We keep the original category order
    supports = [y.value_counts()[letter] for letter in classes] # Re-index them so that they follow the category order
    results = pd.DataFrame({'f1': f1, 'support': supports}, index=classes).sort_values('support')
    
    # Plotting
    fig_dbar, ax_dbar = plt.subplots(1, 2, figsize=(7,4))
    plt.subplots_adjust(wspace=0)
    ax_dbar[0].barh(np.arange(len(results)), results['support'], height=0.7, color='orange')
    ax_dbar[1].barh(np.arange(len(results)), results['f1'], height=0.7)
    ax_dbar[0].invert_xaxis()

    ax_dbar[0].set_yticklabels([])
    ax_dbar[1].set_yticklabels(results.index)
    ax_dbar[1].set_yticks(np.arange(len(results)))

    ax_dbar[0].xaxis.get_major_ticks()[0].label1.set_visible(False)

    ax_dbar[0].set_xlabel('N. items')
    ax_dbar[1].set_xlabel('f1-score')
    fig_dbar.suptitle(title);


show_f1s(y, y_pred_qda, 'QDA classification results')





from sklearn.preprocessing import FunctionTransformer

qda_final = Pipeline([
    ('column_dropper', FunctionTransformer(lambda X: X.drop(['row number', 'interlinear spacing'], axis='columns'))),
    ('standardizer', std_scaler),
    ('qda', QuadraticDiscriminantAnalysis())
])
# qda_final.fit(X, y); Final fit done at the end




















from sklearn.svm import LinearSVC
from sklearn.svm import SVC

lin_svc = LinearSVC(loss='hinge', random_state=random_state, max_iter=100000, verbose=1) # One-vs-Rest
svc = SVC(kernel='linear', random_state=random_state, verbose=True) # One-vs-One

svc_ovr_pipe = Pipeline([
    ('standardizer', StandardScaler()),
    ('svm', lin_svc)
])

svc_ovo_pipe = Pipeline([
    ('standardizer', StandardScaler()),
    ('svm', svc)
])


from sklearn.model_selection import GridSearchCV

param_grid = {
    'svm__C': [0.01, 0.1, 1, 10]
}

svc_ovr_cv = GridSearchCV(
    svc_ovr_pipe,
    param_grid=param_grid,
    scoring='accuracy',
    n_jobs=-1,
    cv=5,
    verbose=1,
)

svc_ovo_cv = GridSearchCV(
    svc_ovo_pipe,
    param_grid=param_grid,
    scoring='accuracy',
    n_jobs=-1,
    cv=5,
    verbose=1
)


ovr_svc_path = 'res/ovr_svc.csv'

if exists(ovr_svc_path):
    ovr_res = pd.read_csv(ovr_svc_path)
    print('Read result from memory')
else:
    svc_ovr_cv.fit(X, y)
    ovr_res = pd.DataFrame(svc_ovr_cv.cv_results_)
    ovr_res.to_csv(ovr_svc_path, index=False)


def gridsearch_or_load(gs, X, y, path):
    """
    Check if the given path exixts. If so, read result file of the gridsearch from memory.
    Otherwise, run the gridsearch and save the results in memory.
    
    Args:
        gs: sk-learn gridsearch instance
        X: training data
        y: target
        path: path of the result file
    
    Returns:
        gs estimator passed
        result pandas dataframe
    """
    if exists(path):
        res = pd.read_csv(path)
        print('Read result from memory')
    else:
        print('Launching gridsearch...')
        gs.fit(X, y)
        res = pd.DataFrame(gs.cv_results_)
        res.to_csv(path, index=False)
        print(f'Results stored at {path}')
        
    return gs, res


ovo_svc_path = 'res/ovo_svc.csv'
svc_ovo_cv, ovo_res = gridsearch_or_load(svc_ovo_cv, X, y, ovo_svc_path)


cv_svm_fig, cv_svm_ax = plt.subplots(figsize=(7,5))
cv_svm_ax.set_xscale('log')
cv_svm_ax.set_title('Grid-search results for linear SVM (OvO, OvR)')
cv_svm_ax.grid(True)

cv_svm_ax.errorbar(x=ovr_res['param_svm__C'], y=ovr_res['mean_test_score'], yerr=ovr_res['std_test_score'], label='OvR')
cv_svm_ax.errorbar(x=ovo_res['param_svm__C'], y=ovo_res['mean_test_score'], yerr=ovo_res['std_test_score'], label='OvO')
cv_svm_ax.hlines(y=baseline, xmin=ovo_res['param_svm__C'].min(), xmax=ovo_res['param_svm__C'].max(), color='#4a4a4a', label='QDA baseline', linestyles='dashed')

for xp, yp, score in zip(ovr_res['param_svm__C'], ovr_res['mean_test_score'], ovr_res['mean_test_score']):
    cv_svm_ax.annotate(text=round(score, 2), xy=(xp,yp))
for xp, yp, score in zip(ovo_res['param_svm__C'], ovo_res['mean_test_score'], ovo_res['mean_test_score']):
    cv_svm_ax.annotate(text=round(score, 2), xy=(xp,yp))

cv_svm_ax.legend()
cv_svm_ax.set_xlabel('C')
cv_svm_ax.set_ylabel('Accuracy');





# Code commented and replaced with static jpg image for faster rendering
# svc_ovo_pipe['svm'].set_params(**{'C':1})
# svc_ovr_pipe['svm'].set_params(**{'C':1})

# svc_ovo_pipe.fit(num_X, num_y)
# svc_ovr_pipe.fit(num_X, num_y);


# figsvmlin, axsvmlin = plt.subplots(1, 2,figsize=(13, 5.1))
# axsvmlin[0].set_title('SVM OvR decision boundary')
# axsvmlin[1].set_title('SVM OvO decision boundary')
# dec_bound(svc_ovr_pipe, num_X, num_y, ax=axsvmlin[0], columns=X.columns, feature_index=(0,8), ylim=(0,1), filler_val=0.5, filler_range=0.7)
# dec_bound(svc_ovo_pipe, num_X, num_y, ax=axsvmlin[1], columns=X.columns, feature_index=(0,8), ylim=(0,1), filler_val=0.5, filler_range=0.7)

















rbf_svc = SVC(kernel='rbf', random_state=random_state, verbose=True) # RBF kernel
poly_svc = SVC(kernel='poly', random_state=random_state, verbose=True) # Polynomial kernel
sigm_svc = SVC(kernel='sigmoid', random_state=random_state, verbose=True) # Polynomial kernel

rbf_params = {
    'svc__gamma': [0.01, 0.1, 1, 10],
    'svc__C': [0.01, 0.1, 1, 10],
}

poly_params = {
    'svc__gamma': [0.01, 0.1, 1],
    'svc__C': [0.01, 0.1, 1],
    'svc__degree': [2, 3]
}


rbf_svc_pipe = Pipeline([ # RBF Pipeline
    ('standardizer', StandardScaler()),
    ('svc', rbf_svc)
])

poly_svc_pipe = Pipeline([ # Poly pipeline
    ('standardizer', StandardScaler()),
    ('svc', poly_svc)
])

sigm_svc_pipe = Pipeline([ # Sigmoid pipeline
    ('standardizer', StandardScaler()),
    ('svc', sigm_svc)
])


svc_rbf_cv = GridSearchCV( # RBF gridsearch
    rbf_svc_pipe,
    param_grid=rbf_params,
    scoring='accuracy',
    n_jobs=-1,
    cv=5,
    verbose=1,
)

svc_poly_cv = GridSearchCV( # Poly gridsearch
    poly_svc_pipe,
    param_grid=poly_params,
    scoring='accuracy',
    n_jobs=-1,
    cv=5,
    verbose=2,
)

svc_sigm_cv = GridSearchCV( # Sigmoid gridsearch
    sigm_svc_pipe,
    param_grid=rbf_params,
    scoring='accuracy',
    n_jobs=-1,
    cv=5,
    verbose=2,
)


rbf_svc_path = 'res/rbf_svc.csv'
svc_rbf_cv, rbf_res = gridsearch_or_load(svc_rbf_cv, X, y, rbf_svc_path)


poly_svc_path = 'res/poly_svc.csv'
svc_poly_cv, poly_res = gridsearch_or_load(svc_poly_cv, X, y, poly_svc_path)


sigm_svc_path = 'res/sigm_svc.csv'
svc_sigm_cv, sigm_res = gridsearch_or_load(svc_sigm_cv, X, y, sigm_svc_path)


def one_result_line_svc(res, label, title, ax):
    """
    Plots an errorbar plot to the given axis using the data contained in res. The output is assume to be the one of an SVM and is plotted across the C values.
    
    Params:
        res (pandas DataFrame): result of a GridSearchCV instance
        label (str): label to show in the legend
        title (str): title of the axis
        ax (plt axis): axis to plot on
    """
    ax.set_xscale('log')
    ax.set_title(title)
    ax.grid(True)
    
    ax.errorbar(x=res['param_svc__C'], y=res['mean_test_score'], yerr=res['std_test_score'], label=label)
    
    # Annotating the maximum value (assuming there is one. Let's be serious, there won't be two)
    max_record = res.loc[res['mean_test_score'].idxmax()]
    xp = max_record['param_svc__C']
    yp = max_record['mean_test_score']
    text = round(max_record['mean_test_score'], 2)
    ax.annotate(text=text, xy=(xp,yp))

    ax.legend()
    ax.set_xlabel('C')


def plot_over_gamma_svc(res, title, ax):
    """
    Construct a result plot of a SVC gridsearch, with C on X axis. Plot one line for each value of gamma.
    
    Params:
        res (pandas DataFrame): result of a GridSearchCV instance
        label (str): label to show in the legend
        title (str): title of the axis
        ax (plt axis): axis to plot on
    """
    for gamma, data in res.groupby('param_svc__gamma'):
        one_result_line_svc(data, f'gamma={gamma}', title, ax)


ker_svm_fig, ker_svm_ax = plt.subplots(2,2, figsize=(17,10.5)) # Inizialize figure

# Plot baseline line
for ax in ker_svm_ax[0]:
    ax.hlines(y=baseline, xmin=rbf_res['param_svc__C'].min(), xmax=rbf_res['param_svc__C'].max(), color='#4a4a4a', label='QDA baseline', linestyles='dashed')
    ax.set_ylabel('Accuracy');

for ax in ker_svm_ax[1]:
    ax.hlines(y=baseline, xmin=poly_res['param_svc__C'].min(), xmax=poly_res['param_svc__C'].max(), color='#4a4a4a', label='QDA baseline', linestyles='dashed')
    ax.set_ylabel('Accuracy');

plot_over_gamma_svc(rbf_res, 'RBF kernel', ker_svm_ax[0][0])
plot_over_gamma_svc(sigm_res, 'Sigmoid kernel', ker_svm_ax[0][1])
plot_over_gamma_svc(poly_res[poly_res['param_svc__degree']==2], 'Poly kernel, degree 2', ker_svm_ax[1][0])
plot_over_gamma_svc(poly_res[poly_res['param_svc__degree']==3], 'Poly kernel, degree 3', ker_svm_ax[1][1])





svm_best_params = literal_eval(rbf_res.loc[rbf_res['rank_test_score'].idxmin()]['params'])
svm_final = rbf_svc_pipe.set_params(**svm_best_params)
# svm_final.fit(X, y); Not needed anymore, final fit is done at the end


# Commented and replaced with static jpg image for faster rendering
# rbf_svc_pipe['svc'].set_params(**{'C':1})
# poly_svc_pipe['svc'].set_params(**{'C':1})
# sigm_svc_pipe['svc'].set_params(**{'C':1})

# rbf_svc_pipe.fit(num_X, num_y)
# poly_svc_pipe.fit(num_X, num_y)
# sigm_svc_pipe.fit(num_X, num_y);


# figsvm, axsvm = plt.subplots(1, 3, figsize=(20, 5.1))
# axsvm[0].set_title('RBF kernel decision boundary')
# axsvm[1].set_title('Poly kernel decision boundary')
# axsvm[2].set_title('Tanh kernel decision boundary')
# dec_bound(rbf_svc_pipe, num_X, num_y, ax=axsvm[0], columns=X.columns, feature_index=(0,8), ylim=(0,1), filler_val=0.5, filler_range=0.7)
# dec_bound(poly_svc_pipe, num_X, num_y, ax=axsvm[1], columns=X.columns, feature_index=(0,8), ylim=(0,1), filler_val=0.5, filler_range=0.7)
# dec_bound(sigm_svc_pipe, num_X, num_y, ax=axsvm[2], columns=X.columns, feature_index=(0,8), ylim=(0,1), filler_val=0.5, filler_range=0.7)








from sklearn.linear_model import LogisticRegression

lr = Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(penalty='none'))])
y_lr = cross_val_predict(lr, X, y)
lr_acc = round(accuracy_score(y, y_lr), 3)
print(f'Initial logistic regression accuracy (cross-validation): {lr_acc}')
print('Other stats:')
print(classification_report(y, y_lr))





show_f1s(y, y_lr, 'Logistic regression f1 scores')


# Both commented for faster rendering
# lr.fit(num_X, num_y);


# figlr, axlr = plt.subplots(figsize=(7.1, 5.1))
# axlr.set_title('Logistic regression decision boundary')
# dec_bound(lr, num_X, num_y, ax=axlr, columns=X.columns, feature_index=(0,8), ylim=(0,2), filler_val=0.5, filler_range=0.7)











from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import SVMSMOTE
from sklearn.model_selection import StratifiedKFold

n_folds = 5
shrink = (n_folds-1)/n_folds

sampling_strategy = {
    'F': int(3000*shrink),
    'E': int(2000*shrink),
    'H': int(1000*shrink),
}

print(f'Sampling strategy: {sampling_strategy}')


smote = SMOTE(sampling_strategy=sampling_strategy, k_neighbors=10, random_state=random_state)


def smote_cross_val(clf, smote, X, y, cv=5):
    """
    Performs cross-validation by using SMOTE.
    
    Params:
        clf: sklearn classifier
        smote: imblearn instance of a smote class
        X: traning data, not oversampled
        y: labels, not oversampled
        cv: number of folds
        
    Returns:
        statistics of f1 and accuracy for each fold aggregated as a pd DataFrame
    """
    skfold = StratifiedKFold(n_splits=n_folds)
    stats = pd.DataFrame(columns=list(y.cat.categories)+['accuracy'])

    for train_index, val_index in skfold.split(X, y):
        X_train, X_val = X.iloc[train_index], X.iloc[val_index]
        y_train, y_val = y.iloc[train_index], y.iloc[val_index]

        # Resample only the current training data with smote
        X_train_r, y_train_r = smote.fit_resample(X_train, y_train)

        clf.fit(X_train_r, y_train_r)
        y_pred = clf.predict(X_val)

        f1s = f1_score(y_val, y_pred, average=None)

        new_row = {label: f1 for label, f1 in zip(y.cat.categories, f1s)}
        new_row['accuracy'] = accuracy_score(y_val, y_pred)
        stats = stats.append(new_row, ignore_index=True)
    
    return stats


smote_res = smote_cross_val(lr, smote, X, y, cv=n_folds)


print(f'Mean results of {n_folds}-fold cross validation after SMOTE oversampling (f1 + accuracy):')
print(smote_res.mean())








svm_smote = SVMSMOTE(sampling_strategy=sampling_strategy, k_neighbors=10, random_state=random_state,
                svm_estimator=SVC(kernel='rbf', C=10, gamma=1) # Using best parameters 
                )


svmsmote_res = smote_cross_val(lr, svm_smote, X, y, cv=n_folds)


print(f'Mean results of {n_folds}-fold cross validation after SMOTE oversampling:')
print(svmsmote_res.mean())








from statsmodels.discrete.discrete_model import MNLogit
from statsmodels.tools.eval_measures import aic

# We need to manually insert a column of 1's
Xc = X.copy()
Xc.insert(0, 'ones', np.ones(len(X), dtype=int))


full_model_name = 'NO_COLUMN'
aic_values = pd.Series(index = [full_model_name] + list(Xc.columns), dtype=float)

for column in [None] + list(Xc.columns):
    # Remove column (if not none)
    if column:
        X_small = Xc.drop(column, axis='columns')
        print(f'Fitting model without predictor {column}')
    else:
        X_small = Xc.copy()
        column = full_model_name
        print(f'Fitting full model')
    
    # Instantiate and fit the model on the reduce dataset
    mnlogit_small_mod = MNLogit(y, X_small)
    with io.capture_output() as captured:
        mnlogit_small_res =mnlogit_small_mod.fit()
    llik_small = mnlogit_small_res.llf
    
    current_aic = aic(llik_small, X_small.shape[0], X_small.shape[1])
    aic_values[column] = current_aic


to_remove = aic_values.idxmin()
print(f'Column to remove to obtain lowest AIC: {to_remove}')
print(aic_values)





aic_fig, aic_ax = plt.subplots(figsize=(7, 5))
aic_plot = aic_values.drop('ones').sort_values()
aic_ax.stem(np.arange(len(aic_plot)), aic_plot, bottom=17000, linefmt='--')
aic_ax.set_xticks(np.arange(len(aic_plot)))
aic_ax.set_xticklabels(aic_plot.index, rotation=45, ha='right')
# plt.xticks(rotation=60);

for xp, yp in enumerate(aic_plot):
    aic_ax.annotate(round(yp), xy=(xp,yp), xytext=(xp-0.4,yp+100))
    
aic_ax.set_title('AIC values for different nested models')
aic_ax.set_xlabel('Excluded predictor')
aic_ax.set_ylabel('AIC')
aic_ax.set_ylim(top=aic_ax.get_ylim()[1]+100);





# final_sampling_strategy = {
#     'F': 3000,
#     'E': 2000,
#     'H': 1000
# }

# print(f'Final sampling strategy: {final_sampling_strategy}')

# final_smote = SMOTE(sampling_strategy=final_sampling_strategy, k_neighbors=10, random_state=random_state)

# X_whole_res, y_whole_res = final_smote.fit_resample(X, y)

# final_lr.fit(X_whole_res, y_whole_res);


final_lr = Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(penalty='none'))])
final_smote_params_lr = {
    'sampling_strategy': {
        'F': 3000,
        'E': 2000,
        'H': 1000
    },
    'k_neighbors': 10,
    'random_state': random_state
}











from sklearn.linear_model import RidgeClassifier
from sklearn.decomposition import PCA

ridge = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', 'passthrough'),
    ('ridge', RidgeClassifier(random_state=random_state))
])

params = {'ridge__alpha': [0.01, 0.1, 1, 10, 100], 'pca': [PCA(6), PCA(7), PCA(8), 'passthrough']}

ridge_gs = GridSearchCV(ridge, param_grid=params,
    scoring='accuracy',
    n_jobs=-1,
    cv=5,
    verbose=2)


ridge_path = 'res/ridge_pca_cv.csv'
ridge_gs, ridge_res = gridsearch_or_load(ridge_gs, X, y, ridge_path)


nopca_ridge_res = ridge_res[ridge_res['param_pca'] == 'passthrough']
best_record = nopca_ridge_res.loc[nopca_ridge_res['mean_test_score'].idxmax()]
best_ridge_params = best_record['params']


print(f'Gridsearch (cross-validation) result:')
print(f'Best accuracy value of {best_record["mean_test_score"]:.3} ± {best_record["std_test_score"]:.1} obtained with lambda {best_record["param_ridge__alpha"]}.')





# Decision boundary visualization, commented and replaced with jpg for faster rendering
# ex_ridge1 = Pipeline([
#     ('scaler', StandardScaler()),
#     ('ridge', RidgeClassifier(alpha=0.1, random_state=random_state))
# ])

# ex_ridge2 = Pipeline([
#     ('scaler', StandardScaler()),
#     ('ridge', RidgeClassifier(alpha=0.2, random_state=random_state))
# ])


# ex_ridge1.fit(num_X, num_y)
# ex_ridge2.fit(num_X, num_y);


# figr, axr = plt.subplots(1, 2, figsize=(15, 5.1))
# axr[0].set_title('gamma=0.1')
# axr[1].set_title('gamma=1')
# figr.suptitle('Ridge Classifier decision boundaries')
# dec_bound(ex_ridge1, num_X, num_y, ax=axr[0], columns=X.columns, feature_index=(0,8), xlim=(-1, 0), ylim=(0.5,1.5), filler_val=0, filler_range=1)
# dec_bound(ex_ridge2, num_X, num_y, ax=axr[1], columns=X.columns, feature_index=(0,8), xlim=(-1, 0), ylim=(0.5,1.5), filler_val=0, filler_range=1)








from sklearn.model_selection import ParameterGrid

strategy = {'H': int(1500*(4/5)), 'E': int(2500*(4/5)), 'F':int(3200*(4/5)), 'X':int(1000*(4/5))}
smote = SMOTE(sampling_strategy=strategy, random_state=random_state)

print(f'Sampling strategy: {strategy}')


# Manually gridsearching
grid = {'ridge__alpha': [0.01, 0.1, 1, 10, 100], 'pca': ['passthrough']}
smote_ridge_res = pd.DataFrame(columns=['param_ridge__alpha', 'mean_test_score', 'std_test_score'])

for params in ParameterGrid(grid):
    ridge.set_params(**params)
    res_folds = smote_cross_val(ridge, smote, X, y, cv=5) # Return results for 5 folds
    # Create new row with: avg acc, std acc, param value
    new_row = {
        'mean_test_score' :res_folds['accuracy'].mean(),
        'std_test_score': res_folds['accuracy'].std(),
        'param_ridge__alpha': params['ridge__alpha']
    }
    smote_ridge_res = smote_ridge_res.append(new_row, ignore_index=True) # add new row to dataframe


best_record = smote_ridge_res.loc[smote_ridge_res['mean_test_score'].idxmax()]
print(f'Gridsearch (cross-validation) result:')
print(f'Best accuracy value of {best_record["mean_test_score"]:.3} ± {best_record["std_test_score"]:.1} obtained with lambda {int(best_record["param_ridge__alpha"])}.')











from sklearn.pipeline import make_pipeline

example_pca = make_pipeline(StandardScaler(), PCA())
example_pca.fit(X)

expl_var_rat = example_pca['pca'].explained_variance_ratio_


var_fig, var_ax = plt.subplots(1, 2, figsize=(14,5))


xticks = np.arange(len(expl_var_rat))
var_ax[0].bar(xticks, expl_var_rat)
var_ax[0].set_xticks(xticks)
var_ax[0].set_xticklabels(xticks+1)
var_ax[0].set_xlabel('Component')
var_ax[0].set_ylabel('Explained variance ratio')
var_ax[0].set_title('Explained variance ratio by component')
var_ax[0].set_axisbelow(True)
var_ax[0].grid()

cum_var = np.cumsum(expl_var_rat)
var_ax[1].plot(cum_var, marker='o')
var_ax[1].vlines(x=xticks, ymin=0, ymax=cum_var, color='black', linestyle='dotted')
var_ax[1].set_axisbelow(True)
var_ax[1].grid()
var_ax[1].set_xlabel('Component')
var_ax[1].set_ylabel('Cumulative xplained variance ratio')
var_ax[1].set_title('Cumulative explained variance ratio by component');





import re
pca_ridge_res = ridge_res[ridge_res['param_pca'] != 'passthrough']
best_record = pca_ridge_res.loc[pca_ridge_res['mean_test_score'].idxmax()]
# Grab number of components with a regex, because it's cool
best_n_comp = re.search(r'n_components=(\d)', str(best_record['param_pca'])).group(1)
print(f'Gridsearch (cross-validation) result:')
print(f'Best accuracy value of {best_record["mean_test_score"]:.3} ± {best_record["std_test_score"]:.1} obtained with lambda {int(best_record["param_ridge__alpha"])} and {best_n_comp} components.')





def one_result_line_ridge(res, label, title, ax, linewidth=2, color=None):
    """
    Plots an errorbar plot to the given axis using the data contained in res. The output is assume to be the one of a RidgeClassifier and is plotted across the alpha values.
    
    Params:
        res (pandas DataFrame): dataframe with the columns mean_test_score, std_test_score, param_ridge__alpha
        label (str): label to show in the legend
        title (str): title of the axis
        ax (plt axis): axis to plot on
    """
    ax.set_xscale('log')
    ax.set_title(title)
    ax.grid(True)
    
    if color:
        ax.errorbar(x=res['param_ridge__alpha'], y=res['mean_test_score'], yerr=res['std_test_score'], label=label, linewidth=linewidth, color=color)
    else:
        ax.errorbar(x=res['param_ridge__alpha'], y=res['mean_test_score'], yerr=res['std_test_score'], label=label, linewidth=linewidth)
    
    # Annotating the maximum value (assuming there is one. Let's be serious, there won't be two)
    max_record = res.loc[res['mean_test_score'].idxmax()]
    xp = max_record['param_ridge__alpha']
    yp = max_record['mean_test_score']
    text = round(max_record['mean_test_score'], 2)
    ax.annotate(text=text, xy=(xp,yp))

    ax.legend()
    ax.set_xlabel('lambda')


ridge_fig, ridge_ax = plt.subplots(figsize=(8, 5))
ridge_ax.set_ylim(0.5, 0.67)
ridge_ax.set_ylabel('Mean accuracy')

ridge_ax.hlines(y=baseline, xmin=nopca_ridge_res['param_ridge__alpha'].min(), xmax=nopca_ridge_res['param_ridge__alpha'].max(), color='#4a4a4a', label='QDA baseline', linestyles='dashed')

one_result_line_ridge(nopca_ridge_res, 'Ridge', 'RidgeClassifier mean accuracy with 5-fold CV', ridge_ax)
one_result_line_ridge(ridge_res[ridge_res['param_pca'].astype(str) == 'PCA(n_components=8)'], 'PCA 8 dim + Ridge', 'RidgeClassifier mean accuracy with 5-fold CV', ridge_ax)
one_result_line_ridge(ridge_res[ridge_res['param_pca'].astype(str) == 'PCA(n_components=7)'], 'PCA 7 dim + Ridge', 'RidgeClassifier mean accuracy with 5-fold CV', ridge_ax)
one_result_line_ridge(ridge_res[ridge_res['param_pca'].astype(str) == 'PCA(n_components=6)'], 'PCA 6 dim + Ridge', 'RidgeClassifier mean accuracy with 5-fold CV', ridge_ax)
one_result_line_ridge(smote_ridge_res, 'SMOTE + Ridge', 'RidgeClassifier mean accuracy with 5-fold CV', ridge_ax)











# final_sampling_strategy = {
#     'F': 3200,
#     'E': 2500,
#     'H': 1500,
#     'X': 1000
# }

# print(f'Final sampling strategy: {final_sampling_strategy}')

# final_smote = SMOTE(sampling_strategy=final_sampling_strategy, random_state=random_state)

# X_whole_res, y_whole_res = final_smote.fit_resample(X, y)

# final_ridge.fit(X_whole_res, y_whole_res);


final_ridge = Pipeline([('scaler', StandardScaler()), ('ridge', RidgeClassifier(alpha=100))])
final_smote_params_ridge = {
    'sampling_strategy': {
        'F': 3200,
        'E': 2500,
        'H': 1500,
        'X': 1000
    },
    'random_state': random_state
}

print(f'Final sampling strategy: {final_smote_params_ridge["sampling_strategy"]}')








from sklearn.neighbors import KNeighborsClassifier

knn_params = {
    'knn__n_neighbors': [1, 2, 3, 5, 10, 15, 20, 25],
    'knn__p': [1, 2, 3]
}
knn = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())])
knn_gs = GridSearchCV(knn, knn_params, cv=5)


knn_path = 'res/knn_gs2.csv'
knn_gs, knn_res = gridsearch_or_load(knn_gs, X, y, knn_path)


best_record = knn_res.loc[knn_res['mean_test_score'].idxmax()]
print(f'Grisearch (cross-validation) result:')
print(f'Best accuracy value of {best_record["mean_test_score"]:.3} ± {best_record["std_test_score"]:.1} obtained with K={int(best_record["param_knn__n_neighbors"])}, p={best_record["param_knn__p"]}.')

best_knn_params = literal_eval(best_record['params'])
knn.set_params(**best_knn_params)
show_f1s(y, cross_val_predict(knn, X, y), 
         f'kNN f1 scores (K={best_knn_params["knn__n_neighbors"]}, p={best_knn_params["knn__p"]})')





def one_result_line_general(res, estimator_name, param_name, label, title, ax, linewidth=2, color=None, scale='log'):
    """
    Plots an errorbar plot to the given axis using the data contained in res.
    
    Params:
        res (pandas DataFrame): dataframe with the columns mean_test_score, std_test_score, param_ridge__alpha
        estimator_name: estimator name in the pipeline, used to reconstruct column name
        param_name: name of the parameter to plot in the x axis
        label (str): label to show in the legend
        title (str): title of the axis
        ax (plt axis): axis to plot on
    """
    if scale:
        ax.set_xscale('log')
    ax.set_title(title)
    ax.grid(True)
    
    if color:
        ax.errorbar(x=res[f'param_{estimator_name}__{param_name}'], y=res['mean_test_score'], yerr=res['std_test_score'], label=label, linewidth=linewidth, color=color)
    else:
        ax.errorbar(x=res[f'param_{estimator_name}__{param_name}'], y=res['mean_test_score'], yerr=res['std_test_score'], label=label, linewidth=linewidth)
    
    # Annotating the maximum value (assuming there is one. Let's be serious, there won't be two)
    max_record = res.loc[res['mean_test_score'].idxmax()]
    xp = max_record[f'param_{estimator_name}__{param_name}']
    yp = max_record['mean_test_score']
    text = round(max_record['mean_test_score'], 2)
    ax.annotate(text=text, xy=(xp,yp))
    
    if label:
        ax.legend()
    ax.set_xlabel(param_name)


knn_fig, knn_ax = plt.subplots(figsize=(9,6))
knn_ax.hlines(y=baseline, xmin=knn_res['param_knn__n_neighbors'].min(), xmax=knn_res['param_knn__n_neighbors'].max(), color='#4a4a4a', label='QDA baseline', linestyles='dashed')
for p in knn_params['knn__p']:
    one_result_line_general(knn_res[knn_res[f'param_knn__p']==p], 'knn', 'n_neighbors', f'p={p}', 'kNN cross-validation accuracy', knn_ax, scale=None)
knn_ax.set_xticks(knn_params['knn__n_neighbors']);
knn_ax.set_ylabel('Mean accuracy');





# Commented and replaced with jpg for faster rendering
# visual_knn = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=1))])
# visual_knn.fit(X.to_numpy(), y.cat.codes.to_numpy());


# knnfig, knnax = plt.subplots(1,2, figsize=(12, 5))

# dec_bound(visual_knn, X.to_numpy(), y.cat.codes.to_numpy(), knnax[0], columns=X.columns, feature_index=(0,1))
# dec_bound(visual_knn, X.to_numpy(), y.cat.codes.to_numpy(), knnax[1], columns=X.columns, feature_index=(2,3))

# knnfig.suptitle('kNN decision boundaries (k=1)');





final_knn = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=1, p=1))])
# final_knn.fit(X, y); Not needed anymore, final fit is done at the end























from sklearn.ensemble import RandomForestClassifier

forest = Pipeline([('scaler', StandardScaler()), ('forest', RandomForestClassifier(random_state=random_state))])
forest_params = {
    'forest__n_estimators': [100, 300, 400, 500],
    'forest__max_depth': [5, 10, 15, 20],
    'forest__min_samples_leaf': [5, 10, 15, 25],
    'forest__min_impurity_decrease': [0.0, 0.1, 0.2, 0.5],
    'forest__oob_score': [True],
    'forest__random_state': [random_state]
}

forest_gs = GridSearchCV(
    forest,
    forest_params,
    n_jobs=-1,
    cv=5,
    verbose=3
)


forest_path = 'res/forest_gs.csv'
print('Performing gridsearch...')
forest_gs, forest_res = gridsearch_or_load(forest_gs, X, y, forest_path)





best_record = forest_res.loc[forest_res['mean_test_score'].idxmax()]
best_params = literal_eval(best_record['params'])

forest.set_params(**best_params)
forest.fit(X,y);


from sklearn.tree import plot_tree
fig_tree, ax_tree = plt.subplots(figsize=(20,9))

plot_tree(
    forest['forest'].estimators_[11],
    max_depth=2,
    feature_names=X.columns,
    filled=True,
    rounded=True,
    fontsize=14
);





# Results display
fig, axes= plt.subplots(1, 4, figsize=(20, 4), sharey='row', subplot_kw={'ylabel': 'Accuracy'})
fig.suptitle('Maximum accuracy by parameter with Radom Forest Classifier', y=1.03)

for param, ax in zip(['n_estimators', 'max_depth', 'min_samples_leaf', 'min_impurity_decrease'], axes):
    agg_res = forest_res.groupby(f'param_forest__{param}').agg({'mean_test_score':'max', 'std_test_score':'mean'})
    agg_res[f'param_forest__{param}'] = agg_res.index
    one_result_line_general(agg_res, 'forest', param, 'RandomForest', f'Best score with {param}', ax, scale=None)
    ax.hlines(y=baseline, xmin=forest_res[f'param_forest__{param}'].min(), xmax=forest_res[f'param_forest__{param}'].max(), color='#4a4a4a', label='QDA baseline', linestyles='dashed')
    ax.legend()
    
# Printing best absolute result


print(f'Best accuracy value of {best_record["mean_test_score"]:.3} ± {best_record["std_test_score"]:.1} obtained with parameters:')
for param, val in best_params.items():
    p_name = re.search(r'__(.*)', param).group(1)
    print(f'{p_name} = {val}')





# Replanced with static jpg image for faster rendering
# num_y = y.cat.codes.to_numpy()
# num_X = X.to_numpy()
# forest.fit(num_X, num_y)


# fig, axes = plt.subplots(1, 3, figsize=(20,5), subplot_kw={})
# fig.suptitle('Decision regions of Random Forest with different couples of features')

# fts_indexes = [(3,1), (4,1), (0,1)]
# for ax, idxes in zip(axes, fts_indexes):
#     dec_bound(forest, num_X, num_y, ax=ax, columns=X.columns, feature_index=idxes)











ft_importances = pd.Series(
    data=forest['forest'].feature_importances_,
    index=X.columns
).sort_values(ascending=False)


ft_importances = pd.DataFrame({
    'importance': forest['forest'].feature_importances_,
    'type': ['page', 'page', 'page', 'column', 'column', 'row', 'row', 'row', 'row']
}, index=X.columns).sort_values('importance', ascending=False)


fig, aximp = plt.subplots(figsize=(7, 5), 
                          subplot_kw={
                              'title': 'Feature importance according to Random Forest',
                              'xlabel': 'Feature',
                              'ylabel': 'Importance',
                              'xticks': range(len(ft_importances)),
                          })

colors = {
    'page': '#73c94f',
    'column': '#4f63bd',
    'row': '#fcba03'
}

for type_col, color in colors.items():
    mask = (ft_importances['type'] == type_col)
    data = ft_importances[mask]
    pos = np.argwhere(mask).flatten()
    aximp.bar(pos, data['importance'], color=color, label=type_col)
    
aximp.legend()
aximp.set_xticklabels(ft_importances.index, rotation=40, ha='right');








final_forest = forest.set_params(**best_params)
final_forest.fit(X, y);


print(f'Out-of-bag score of the final Random Forest: {final_forest["forest"].oob_score_:.3}')








from sklearn.utils import resample
final_result_path = 'res/final4.csv'

X_test = avila_test.drop('monk', axis='columns')
y_test = avila_test['monk']
print(f'Test dataset imported.')
print(f'Test set size: {len(X_test)}')





outliers_test_mask = (isol_for.predict(X_test) == -1)
n_out_test = np.sum(outliers_test_mask)
print(f'Isolation Forest found {n_out_test}.')

to_keep_test = np.logical_not(outliers_test_mask)
X_test = X_test[to_keep_test]
y_test = y_test[to_keep_test]

print(f'Removed {n_out_test} outliers.')
print(f'New test dataset size: {len(X_test)}')





# Getting all the final results
final_models = [qda_final, svm_final, final_lr, final_ridge, final_knn, final_forest]
names = ['QDA', 'SVM', 'Logistic Regression', 'Ridge Classifier', 'kNN', 'Random Forest']

n_bootstraps = 5


if exists(final_result_path):
    final_score = pd.read_csv(final_result_path, index_col=0)
else:
    final_score = pd.DataFrame(index=names)

    for i in range(n_bootstraps):
        print(f'Bootstrap {i+1}')
        X_train_boot, y_train_boot = resample(X, y, random_state=i, stratify=y)
        # Save scores in dataframe. Rows: model. Columns: rounds, then mean and std
        scores_boot = pd.Series(index=names)
        for model, name in zip(final_models, names):
            # Logistic reg and ridge need SMOTE on training set. QDA needs two attributes dropped
            if name == 'Logistic Regression':
                print('(Resampling...)', end=' ')
                final_smote = SMOTE(**final_smote_params_lr)
                X_train_boot, y_train_boot = final_smote.fit_resample(X_train_boot, y_train_boot)
            elif name == 'Ridge Classifier':
                print('(Resampling...)', end=' ')
                final_smote = SMOTE(**final_smote_params_ridge)
                X_train_boot, y_train_boot = final_smote.fit_resample(X_train_boot, y_train_boot)
            
            # Training the model (whatever it is) and predicting on test
    
            model.fit(X_train_boot, y_train_boot)
            y_pred = model.predict(X_test)
            score = accuracy_score(y_test, y_pred)
            scores_boot[name] = score
            print(f'\t - {name} scored {score:.3}')

        final_score[f'round_{i}'] = scores_boot
    
    # Save final scores to disk
    final_score.to_csv(final_result_path)


verdict = final_score.copy()
verdict['mean'] = final_score.mean(axis=1)
verdict['std'] = final_score.std(axis=1)


fig, final_ax = plt.subplots(figsize=(8, 6),
                             subplot_kw={
                                 'title': 'Performance on test set',
                                 'ylabel': 'Accuracy',
                                 'xticks': range(len(verdict))
                             })

for i, (index, row) in enumerate(verdict.iterrows()):
    final_ax.errorbar(
        #x=range(len(verdict)),
        x=i,
        y=row['mean'],
        yerr=row['std'],
        marker=' ',
        linestyle=' ',
        fillstyle='none',
        elinewidth=2.5,
        markersize=10,
        markeredgewidth=2,
        color='black'
        
    )
    
    final_ax.bar(
        #x=range(len(verdict)),
        x=i,
        height=row['mean']
    )
    
final_ax.grid()
final_ax.set_axisbelow(True)
final_ax.set_xticklabels(['SVM RBF' if name=='SVM' else name for name in names], rotation=35, ha='right');
final_ax.set_ylim(top=1.1)

for xp, yp in enumerate(verdict['mean']):
    final_ax.annotate(f'{yp:.2f}', xy=(xp,yp), xytext=(xp-0.22,yp+0.03), fontsize='x-large')

print('Final test accuracy:\n')
for idx, row in verdict.iterrows():
    print(f'{idx}: {row["mean"]:.2} ± {row["std"]:.2}')






